{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airbus_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lTaF_aP4tYL1",
        "rXmvk6x_tvtS",
        "mxk2KgmqmztA"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOP8MDx2+Oblv5ian597S1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mziad97/Airbus-semantic-segmentation-pytorch/blob/main/Airbus_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z51lHzbOX64D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copyfile\n",
        "import copy\n",
        "import time \n",
        "\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data from Kaggle"
      ],
      "metadata": {
        "id": "iyQzeOqPzWUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "0l7r6jKTzgOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a608a4-bf69-495b-823d-af8cb9a8f1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 1.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=a426e7752d533bce3bdda57a348b80937626a0e89238945dc239ab2dc31f50c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ('train_v2' not in os.listdir('.')):\n",
        "  ! kaggle competitions download -c airbus-ship-detection "
      ],
      "metadata": {
        "id": "XydsIkY7znA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b648ded7-ba51-4d96-86b0-6d97b29b488b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading airbus-ship-detection.zip to /content\n",
            "100% 28.6G/28.6G [10:59<00:00, 76.6MB/s]\n",
            "100% 28.6G/28.6G [10:59<00:00, 46.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "    zipObj.extract('train_ship_segmentations_v2.csv')"
      ],
      "metadata": {
        "id": "pr2B6UVr5Qjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = pd.read_csv('train_ship_segmentations_v2.csv', index_col=0).dropna().reset_index()\n",
        "\n",
        "segments = segments.groupby(\"ImageId\")[['EncodedPixels']].agg(lambda rle_codes: ' '.join(rle_codes)).reset_index()\n",
        "\n",
        "segments = segments[:7000]"
      ],
      "metadata": {
        "id": "YA7zOUtb5yhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "  for file in segments['ImageId'].values:\n",
        "      file = os.path.join('train_v2', file)\n",
        "      zipObj.extract(file)\n",
        "\n",
        "! rm airbus-ship-detection.zip"
      ],
      "metadata": {
        "id": "349Ll0gs55Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, val_paths = train_test_split(segments, train_size=0.85, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "D5Z21lJ67qA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of train set: {len(train_paths)}\")\n",
        "print(f\"The number of test set: {len(val_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq6oDfc47sHE",
        "outputId": "a4e27ec2-99ca-41d0-b9b0-616296a00ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train set: 5950\n",
            "The number of test set: 1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = train_paths.reset_index(drop=True)\n",
        "val_paths = val_paths.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_FE9zNlI7rhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "move the images from 'train_v2' to 'data'where there are a folder for train and val\n",
        "\"\"\"\n",
        "! rm -r data\n",
        "! mkdir data\n",
        "! mkdir data/train\n",
        "! mkdir data/val\n",
        "\n",
        "SOURCE = 'train_v2'\n",
        "train_path = './data/train'\n",
        "val_path = './data/val'\n",
        "\n",
        "def build_data_dir(SOURCE, DEST, files):\n",
        "  for filename in files:\n",
        "    src = os.path.join(SOURCE, filename)\n",
        "    dest = os.path.join(DEST, filename)\n",
        "    copyfile(src, dest)\n",
        "\n",
        "build_data_dir(SOURCE, val_path, val_paths['ImageId'].values)\n",
        "build_data_dir(SOURCE, train_path, train_paths['ImageId'].values)"
      ],
      "metadata": {
        "id": "Jv0oRnZvqWTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec08822d-4674-4144-83af-1f6d6ab97253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Model"
      ],
      "metadata": {
        "id": "lTaF_aP4tYL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "    super().__init__()\n",
        "    self.conv2d = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size) , padding='same'),\n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), padding='same'),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv2d(x)\n",
        "\n",
        "class Encoder_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, pool_size=(2,2) , dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
        "    self.max_pool = nn.MaxPool2d(pool_size)\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f = self.conv2d_block(x)\n",
        "    P = self.max_pool(f)\n",
        "    P = self.dropout(P)\n",
        "\n",
        "    return f, P\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder_block_1 = Encoder_Block(in_channels=3, out_channels=64, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_2 = Encoder_Block(in_channels=64, out_channels=128, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_3 = Encoder_Block(in_channels=128, out_channels=256, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_4 = Encoder_Block(in_channels=256, out_channels=512, pool_size=(2,2) , dropout=0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f1, P1 = self.encoder_block_1(x)\n",
        "    f2, P2 = self.encoder_block_2(P1)\n",
        "    f3, P3 = self.encoder_block_3(P2)\n",
        "    f4, P4 = self.encoder_block_4(P3)\n",
        "\n",
        "    return P4, (f1, f2, f3, f4)\n",
        "\n",
        "class Bottle_Neck(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2d_block = Conv2d_Block(in_channels=512, out_channels=1024)\n",
        "\n",
        "  def forward(self, x):\n",
        "    bottleneck = self.conv2d_block(x)\n",
        "    \n",
        "    return bottleneck\n",
        "\n",
        "class Decoder_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(3, 3), strides=2, padding=1, output_padding=1, dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "    self.u = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                                stride=strides, padding=padding, output_padding=output_padding)\n",
        "    # self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
        "\n",
        "  def forward(self, x, conv_outputs):\n",
        "    c = torch.cat([self.u(x), conv_outputs], 1)\n",
        "    # c = self.dropout(c),\n",
        "    c = self.conv2d_block(c)\n",
        "    \n",
        "    return c\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, last_out_channels):\n",
        "    super().__init__()\n",
        "    self.decoder_block_1 = Decoder_Block(in_channels=1024, out_channels=512)\n",
        "    self.decoder_block_2 = Decoder_Block(in_channels=512, out_channels=256)\n",
        "    self.decoder_block_3 = Decoder_Block(in_channels=256, out_channels=128)\n",
        "    self.decoder_block_4 = Decoder_Block(in_channels=128, out_channels=64)    \n",
        "    self.conv2d_output = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=64, out_channels=last_out_channels, kernel_size=(1,1)),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, convs):\n",
        "    f1, f2, f3, f4 = convs\n",
        "    c6 = self.decoder_block_1(x, f4)\n",
        "    c7 = self.decoder_block_2(c6, f3)\n",
        "    c8 = self.decoder_block_3(c7, f2)\n",
        "    c9 = self.decoder_block_4(c8, f1)\n",
        "    outputs = self.conv2d_output(c9)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "LAST_OUT_CHANNELS = 1\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, LAST_OUT_CHANNELS):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder(LAST_OUT_CHANNELS)\n",
        "    self.bottle_neck = Bottle_Neck()\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_output, convs = self.encoder(x)\n",
        "    bottleneck = self.bottle_neck(encoder_output)\n",
        "    outputs = self.decoder(bottleneck, convs)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "R5aF6pGEbIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pipeline"
      ],
      "metadata": {
        "id": "rXmvk6x_tvtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.utils\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "WxxfG8eFtV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  \"\"\"\n",
        "  create a custom dataset\n",
        "  \n",
        "  Args:\n",
        "    images_dir: the path that contains all images\n",
        "    annotations: a dataframe, where each record(filename, Run-length encoding)\n",
        "    transform: transformations on input images (resizing, normalization, augmentation, etc)\n",
        "    target_transform: transform run-length encoding to segmentation mask\n",
        "  \n",
        "  returns:\n",
        "    Dataset object\n",
        "  \"\"\"\n",
        "  def __init__(self, images_dir, annotations, transform=None, target_transform=None):\n",
        "    self.annotations = annotations\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, self.annotations.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    segmentation = self.annotations.iloc[idx, 1]\n",
        "\n",
        "    if(self.transform):\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if(self.target_transform):\n",
        "      segmentation = self.target_transform(segmentation)\n",
        "\n",
        "    return image, segmentation"
      ],
      "metadata": {
        "id": "ZNKxR8_YkKqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rle_to_pixels(rle_code):\n",
        "  \"\"\"\n",
        "  Decode the segmentation mask from run-length-encoding\n",
        "  1.convert the string into tokens that represents start and length\n",
        "  2. unravel the the pixels range(start, start+length)\n",
        "  3. map the pixel to 2D, whose shape is 768*768\n",
        "  \"\"\"\n",
        "  rle_code = [int(i) for i in rle_code.split()]\n",
        "  pixels = [(pixel_position % 768, pixel_position // 768) \n",
        "                for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n",
        "                for pixel_position in range(start, start + length)]\n",
        "  return pixels\n",
        "\n",
        "def pixels_to_mask(pixels):\n",
        "  \"\"\"\n",
        "  project the pixels onto a canvas of 768*768\n",
        "\n",
        "  1. create a sparse tensor with the decoded pixels\n",
        "  2. change to dense tensor\n",
        "  3. add a dimension -> to make the dimensions: (768,768,1)\n",
        "  \"\"\"\n",
        "  canvas = np.zeros((768, 768))\n",
        "\n",
        "  canvas[tuple(zip(*pixels))] = 1\n",
        "\n",
        "  return torch.as_tensor(np.expand_dims(canvas, axis=0), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "nnZpFYSMv0yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        # transforms.RandomResizedCrop(224),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "target_transform = {\n",
        "    'train': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ])\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ])\n",
        "}\n",
        "\n",
        "paths = {'train':'data/train', 'val':'data/val'}"
      ],
      "metadata": {
        "id": "Ucy1kYBtBvHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {'train': CustomDataset(paths['train'], train_paths, transform['train'], target_transform['train']),\n",
        "            'val': CustomDataset(paths['val'], val_paths, transform['val'], target_transform['val'])\n",
        "            }\n",
        "\n",
        "dataloaders = {'train': torch.utils.data.DataLoader(datasets['train'], batch_size=16, shuffle=True),\n",
        "               'val':torch.utils.data.DataLoader(datasets['val'], batch_size=16) \n",
        "               }\n",
        "               \n",
        "dataset_sizes = {'train': len(train_paths)*128*128,\n",
        "                 'val': len(val_paths)*128*128\n",
        "                 }"
      ],
      "metadata": {
        "id": "p6fbyQqnyans"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "MK3rPYGyywuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "mxk2KgmqmztA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, scheduler, EPOCHS):\n",
        "  \"\"\"\n",
        "  for i in epochs:\n",
        "    train phase:\n",
        "      pass the inputs through the net\n",
        "      compute the loss\n",
        "      computer gradients\n",
        "      update weights\n",
        "    \n",
        "    val phase:\n",
        "      passe the inputs through the net\n",
        "      compute the loss\n",
        "\n",
        "  returns:\n",
        "    save the weights of the best model\n",
        "  \"\"\"\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f'epoch: {epoch}/{EPOCHS}:')\n",
        "    print('-'*10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if(phase =='train'):\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_pixels_count = np.prod(list(inputs.shape))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # _, preds = torch.max(outputs, 1, keepdim=True)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          if(phase == 'train'):\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_pixels_count \n",
        "        \n",
        "        running_corrects += (labels.data == (outputs > 0.5)).sum()\n",
        "\n",
        "      if(phase == 'train'):\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]   # / total_pixels_dataset\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]  # / total_pixels_dataset\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      if(phase == 'val' and epoch_acc > best_acc):\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "    print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hBeGxjocm1Up"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = UNet(1)\n",
        "\n",
        "unet = unet.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(unet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "BYwx6vXhST3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(unet, optimizer, criterion, exp_lr_scheduler, EPOCHS)"
      ],
      "metadata": {
        "id": "V84sskZ_nvrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(unet, 'unet.pt')"
      ],
      "metadata": {
        "id": "ZBykUMU1TC3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SegFormer"
      ],
      "metadata": {
        "id": "-zl1rQ40LMxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "tyDWMtZNLcII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3621f83-4abf-42d0-f7c0-562e8b33ed21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 27.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
        "\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "FCNxjPxgLOT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pipeline"
      ],
      "metadata": {
        "id": "rJOSxD4S43OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = SegformerFeatureExtractor(align=False, reduce_labels=False, reduce_zero_labels=False)"
      ],
      "metadata": {
        "id": "r1U9AYKBLkCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rle_to_pixels(rle_code):\n",
        "  \"\"\"\n",
        "  Decode the segmentation mask from run-length-encoding\n",
        "  1.convert the string into tokens that represents start and length\n",
        "  2. unravel the the pixels range(start, start+length)\n",
        "  3. map the pixel to 2D, whose shape is 768*768\n",
        "  \"\"\"\n",
        "  rle_code = [int(i) for i in rle_code.split()]\n",
        "  pixels = [(pixel_position % 768, pixel_position // 768) \n",
        "                for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n",
        "                for pixel_position in range(start, start + length)]\n",
        "  return pixels\n",
        "\n",
        "def pixels_to_mask(pixels):\n",
        "  \"\"\"\n",
        "  project the pixels onto a canvas of 768*768\n",
        "\n",
        "  1. create a sparse tensor with the decoded pixels\n",
        "  2. change to dense tensor\n",
        "  3. add a dimension -> to make the dimensions: (768,768,1)\n",
        "  \"\"\"\n",
        "  canvas = np.zeros((768, 768))\n",
        "\n",
        "  canvas[tuple(zip(*pixels))] = 255\n",
        "\n",
        "  return canvas"
      ],
      "metadata": {
        "id": "iakU0b9AzP0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_transform = transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask\n",
        "    ])\n",
        "\n",
        "paths = {'train':'data/train', 'val':'data/val'}"
      ],
      "metadata": {
        "id": "piG7mtUtzP0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatasetForTransformer(Dataset):\n",
        "  \"\"\"\n",
        "  create a custom dataset\n",
        "  \n",
        "  Args:\n",
        "    images_dir: the path that contains all images\n",
        "    annotations: a dataframe, where each record(filename, Run-length encoding)\n",
        "    transform: transformations on input images (resizing, normalization, augmentation, etc)\n",
        "    target_transform: transform run-length encoding to segmentation mask\n",
        "  \n",
        "  returns:\n",
        "    Dataset object\n",
        "  \"\"\"\n",
        "  def __init__(self, images_dir, annotations, feature_extractor, transform=None, target_transform=None):\n",
        "    self.annotations = annotations\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.feature_extractor = feature_extractor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, self.annotations.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    segmentation = self.annotations.iloc[idx, 1]\n",
        "\n",
        "    if(self.transform):\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if(self.target_transform):\n",
        "      segmentation = self.target_transform(segmentation)\n",
        "\n",
        "    encoded_inputs = self.feature_extractor(image, segmentation, return_tensors='pt')\n",
        "\n",
        "    for k, v in encoded_inputs.items():\n",
        "      encoded_inputs[k].squeeze_()\n",
        "\n",
        "    return encoded_inputs"
      ],
      "metadata": {
        "id": "znmat5XhLjrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {'train': CustomDatasetForTransformer(paths['train'], train_paths, feature_extractor, target_transform=target_transform),\n",
        "            'val': CustomDatasetForTransformer(paths['val'], val_paths, feature_extractor, target_transform=target_transform)\n",
        "            }\n",
        "\n",
        "dataloaders = {'train': torch.utils.data.DataLoader(datasets['train'], batch_size=8, shuffle=True),\n",
        "               'val':torch.utils.data.DataLoader(datasets['val'], batch_size=16) \n",
        "               }\n",
        "               \n",
        "dataset_sizes = {'train': len(train_paths)*128*128,\n",
        "                 'val': len(val_paths)*128*128\n",
        "                 }"
      ],
      "metadata": {
        "id": "QSPb4FGw5F_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloaders['train']))"
      ],
      "metadata": {
        "id": "fV7tZfVf52_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['pixel_values'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Qg1aGX5-gk",
        "outputId": "ae8623a1-a5e5-487a-9a07-4bd8466ac81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 3, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['labels'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHhjrE_d7KHF",
        "outputId": "6f501045-d738-4757-bc05-a3a6afe1d988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['labels'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9InDJXk-aiI",
        "outputId": "0628c671-e2a1-46f2-e426-f642b02c3856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "I2YzZY3q-5YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {'background':0, 'ship':1}\n",
        "id2label = {0:'backgroumd', 1:'ship'}\n",
        "num_labels = 1\n",
        "\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\", ignore_mismatched_sizes=True,\n",
        "                                                         num_labels=num_labels, id2label=id2label, label2id=label2id,\n",
        "                                                         reshape_last_stage=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McebzX9t5-U9",
        "outputId": "9888c29a-f27d-4b2d-8d83-3ee77276df8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.classifier.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.num_batches_tracked']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzhskhNd_V05",
        "outputId": "3d7fba2b-dfea-40c0-c24a-d8f97c45b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SegformerForSemanticSegmentation(\n",
              "  (segformer): SegformerModel(\n",
              "    (encoder): SegformerEncoder(\n",
              "      (patch_embeddings): ModuleList(\n",
              "        (0): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
              "          (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block): ModuleList(\n",
              "        (0): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
              "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "              )\n",
              "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
              "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "              )\n",
              "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
              "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "              )\n",
              "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
              "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "              )\n",
              "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
              "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
              "              )\n",
              "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
              "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
              "              )\n",
              "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
              "              )\n",
              "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
              "              )\n",
              "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): ModuleList(\n",
              "        (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decode_head): SegformerDecodeHead(\n",
              "    (linear_c): ModuleList(\n",
              "      (0): SegformerMLP(\n",
              "        (proj): Linear(in_features=32, out_features=256, bias=True)\n",
              "      )\n",
              "      (1): SegformerMLP(\n",
              "        (proj): Linear(in_features=64, out_features=256, bias=True)\n",
              "      )\n",
              "      (2): SegformerMLP(\n",
              "        (proj): Linear(in_features=160, out_features=256, bias=True)\n",
              "      )\n",
              "      (3): SegformerMLP(\n",
              "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (activation): ReLU()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=0.00006)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo4z7sDiBUC_",
        "outputId": "86081a4c-cb94-47e7-f83e-8ecb698c9e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SegformerForSemanticSegmentation(\n",
              "  (segformer): SegformerModel(\n",
              "    (encoder): SegformerEncoder(\n",
              "      (patch_embeddings): ModuleList(\n",
              "        (0): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
              "          (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): SegformerOverlapPatchEmbeddings(\n",
              "          (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (block): ModuleList(\n",
              "        (0): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
              "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "              )\n",
              "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
              "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "              )\n",
              "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
              "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "              )\n",
              "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
              "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "              )\n",
              "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
              "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
              "              )\n",
              "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
              "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
              "              )\n",
              "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ModuleList(\n",
              "          (0): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
              "              )\n",
              "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SegformerLayer(\n",
              "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SegformerAttention(\n",
              "              (self): SegformerEfficientSelfAttention(\n",
              "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): SegformerSelfOutput(\n",
              "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath()\n",
              "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): SegformerMixFFN(\n",
              "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (dwconv): SegformerDWConv(\n",
              "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
              "              )\n",
              "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): ModuleList(\n",
              "        (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decode_head): SegformerDecodeHead(\n",
              "    (linear_c): ModuleList(\n",
              "      (0): SegformerMLP(\n",
              "        (proj): Linear(in_features=32, out_features=256, bias=True)\n",
              "      )\n",
              "      (1): SegformerMLP(\n",
              "        (proj): Linear(in_features=64, out_features=256, bias=True)\n",
              "      )\n",
              "      (2): SegformerMLP(\n",
              "        (proj): Linear(in_features=160, out_features=256, bias=True)\n",
              "      )\n",
              "      (3): SegformerMLP(\n",
              "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (activation): ReLU()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = dataloaders['train']"
      ],
      "metadata": {
        "id": "lE6u-6HNiUXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "   print(\"Epoch:\", epoch)\n",
        "   for batch in tqdm(train_dataloader):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # evaluate\n",
        "        # upscale from nbatch*nchannels*128*128 -> nbatch*nchannels*512*512\n",
        "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        # nbatch*nchannels*512*512 -> nbatch*512*512 (get the class of the highest probaility)\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "        \n",
        "        # mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "        pred_labels = predicted.detach().cpu().numpy()\n",
        "        true_labels = labels.detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "        print(\"Pixel-wise accuracy:\", accuracy)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        print(\"Loss:\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "t8IRL2mpjD_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'airbus_segformer.pt')"
      ],
      "metadata": {
        "id": "tGKBfQhWluv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}