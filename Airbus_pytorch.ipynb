{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airbus_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjoCgtthNv9p+JhF0ZMNwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mziad97/Airbus-semantic-segmentation-pytorch/blob/main/Airbus_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plan"
      ],
      "metadata": {
        "id": "gcVBaDIa3PoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* build the model\n",
        "* input dimension ? D\n",
        "* concatenate layer? D\n",
        "* make the transformation with CPU, train with GPU? D\n",
        "* data dir: data -> train, test? D\n",
        "* train\n",
        "* augmentation? \n",
        "* use more images?\n",
        "\n"
      ],
      "metadata": {
        "id": "DFawKm8a3TSE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z51lHzbOX64D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copyfile\n",
        "import copy\n",
        "import time \n",
        "\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Model"
      ],
      "metadata": {
        "id": "lTaF_aP4tYL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "    super().__init__()\n",
        "    self.conv2d = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size) , padding='same'),\n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), padding='same'),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv2d(x)\n",
        "\n",
        "class Encoder_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, pool_size=(2,2) , dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
        "    self.max_pool = nn.MaxPool2d(pool_size)\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f = self.conv2d_block(x)\n",
        "    P = self.max_pool(f)\n",
        "    P = self.dropout(P)\n",
        "\n",
        "    return f, P\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder_block_1 = Encoder_Block(in_channels=3, out_channels=64, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_2 = Encoder_Block(in_channels=64, out_channels=128, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_3 = Encoder_Block(in_channels=128, out_channels=256, pool_size=(2,2) , dropout=0.3)\n",
        "    self.encoder_block_4 = Encoder_Block(in_channels=256, out_channels=512, pool_size=(2,2) , dropout=0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f1, P1 = self.encoder_block_1(x)\n",
        "    f2, P2 = self.encoder_block_2(P1)\n",
        "    f3, P3 = self.encoder_block_3(P2)\n",
        "    f4, P4 = self.encoder_block_4(P3)\n",
        "\n",
        "    return P4, (f1, f2, f3, f4)\n",
        "\n",
        "class Bottle_Neck(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2d_block = Conv2d_Block(in_channels=512, out_channels=1024)\n",
        "\n",
        "  def forward(self, x):\n",
        "    bottleneck = self.conv2d_block(x)\n",
        "    \n",
        "    return bottleneck\n",
        "\n",
        "class Decoder_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(3, 3), strides=2, padding=1, output_padding=1, dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "    self.u = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                                stride=strides, padding=padding, output_padding=output_padding)\n",
        "    # self.dropout = nn.Dropout(p=dropout, inplace=True)\n",
        "\n",
        "  def forward(self, x, conv_outputs):\n",
        "    c = torch.cat([self.u(x), conv_outputs], 1)\n",
        "    # c = self.dropout(c),\n",
        "    c = self.conv2d_block(c)\n",
        "    \n",
        "    return c\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, last_out_channels):\n",
        "    super().__init__()\n",
        "    self.decoder_block_1 = Decoder_Block(in_channels=1024, out_channels=512)\n",
        "    self.decoder_block_2 = Decoder_Block(in_channels=512, out_channels=256)\n",
        "    self.decoder_block_3 = Decoder_Block(in_channels=256, out_channels=128)\n",
        "    self.decoder_block_4 = Decoder_Block(in_channels=128, out_channels=64)    \n",
        "    self.conv2d_output = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=64, out_channels=last_out_channels, kernel_size=(1,1)),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, convs):\n",
        "    f1, f2, f3, f4 = convs\n",
        "    c6 = self.decoder_block_1(x, f4)\n",
        "    c7 = self.decoder_block_2(c6, f3)\n",
        "    c8 = self.decoder_block_3(c7, f2)\n",
        "    c9 = self.decoder_block_4(c8, f1)\n",
        "    outputs = self.conv2d_output(c9)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "LAST_OUT_CHANNELS = 1\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, LAST_OUT_CHANNELS):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder(LAST_OUT_CHANNELS)\n",
        "    self.bottle_neck = Bottle_Neck()\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_output, convs = self.encoder(x)\n",
        "    bottleneck = self.bottle_neck(encoder_output)\n",
        "    outputs = self.decoder(bottleneck, convs)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "R5aF6pGEbIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data from Kaggle"
      ],
      "metadata": {
        "id": "iyQzeOqPzWUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "0l7r6jKTzgOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17745d4-dc23-408b-d9b4-b0efe0391734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=abea2f8bcfedad38c7ce07a706959431067a8f0f4d57be1aa5c26bfb00cc45aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ('train_v2' not in os.listdir('.')):\n",
        "  ! kaggle competitions download -c airbus-ship-detection "
      ],
      "metadata": {
        "id": "XydsIkY7znA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b226b9d-6513-44db-cc7d-9b4879c32992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading airbus-ship-detection.zip to /content\n",
            "100% 28.6G/28.6G [11:15<00:00, 52.6MB/s]\n",
            "100% 28.6G/28.6G [11:16<00:00, 45.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "    zipObj.extract('train_ship_segmentations_v2.csv')"
      ],
      "metadata": {
        "id": "pr2B6UVr5Qjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = pd.read_csv('train_ship_segmentations_v2.csv', index_col=0).dropna().reset_index()\n",
        "\n",
        "segments = segments.groupby(\"ImageId\")[['EncodedPixels']].agg(lambda rle_codes: ' '.join(rle_codes)).reset_index()\n",
        "\n",
        "segments = segments[:7000]"
      ],
      "metadata": {
        "id": "YA7zOUtb5yhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "  for file in segments['ImageId'].values:\n",
        "      file = os.path.join('train_v2', file)\n",
        "      zipObj.extract(file)\n",
        "\n",
        "! rm airbus-ship-detection.zip"
      ],
      "metadata": {
        "id": "349Ll0gs55Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, val_paths = train_test_split(segments, train_size=0.85, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "D5Z21lJ67qA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of train set: {len(train_paths)}\")\n",
        "print(f\"The number of test set: {len(val_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq6oDfc47sHE",
        "outputId": "704defb3-646a-4b27-ee3c-69a5f5dde293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train set: 5950\n",
            "The number of test set: 1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = train_paths.reset_index(drop=True)\n",
        "val_paths = val_paths.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_FE9zNlI7rhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "move the images from 'train_v2' to 'data'where there are a folder for train and val\n",
        "\"\"\"\n",
        "! rm -r data\n",
        "! mkdir data\n",
        "! mkdir data/train\n",
        "! mkdir data/val\n",
        "\n",
        "SOURCE = 'train_v2'\n",
        "train_path = './data/train'\n",
        "val_path = './data/val'\n",
        "\n",
        "def build_data_dir(SOURCE, DEST, files):\n",
        "  for filename in files:\n",
        "    src = os.path.join(SOURCE, filename)\n",
        "    dest = os.path.join(DEST, filename)\n",
        "    copyfile(src, dest)\n",
        "\n",
        "build_data_dir(SOURCE, val_path, val_paths['ImageId'].values)\n",
        "build_data_dir(SOURCE, train_path, train_paths['ImageId'].values)"
      ],
      "metadata": {
        "id": "Jv0oRnZvqWTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "rXmvk6x_tvtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.utils\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "WxxfG8eFtV3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  \"\"\"\n",
        "  create a custom dataset\n",
        "  \n",
        "  Args:\n",
        "    images_dir: the path that contains all images\n",
        "    annotations: a dataframe, where each record(filename, Run-length encoding)\n",
        "    transform: transformations on input images (resizing, normalization, augmentation, etc)\n",
        "    target_transform: transform run-length encoding to segmentation mask\n",
        "  \n",
        "  returns:\n",
        "    Dataset object\n",
        "  \"\"\"\n",
        "  def __init__(self, images_dir, annotations, transform=None, target_transform=None):\n",
        "    self.annotations = annotations\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, self.annotations.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    segmentation = self.annotations.iloc[idx, 1]\n",
        "\n",
        "    if(self.transform):\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if(self.target_transform):\n",
        "      segmentation = self.target_transform(segmentation)\n",
        "\n",
        "    return image, segmentation"
      ],
      "metadata": {
        "id": "ZNKxR8_YkKqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rle_to_pixels(rle_code):\n",
        "  \"\"\"\n",
        "  Decode the segmentation mask from run-length-encoding\n",
        "  1.convert the string into tokens that represents start and length\n",
        "  2. unravel the the pixels range(start, start+length)\n",
        "  3. map the pixel to 2D, whose shape is 768*768\n",
        "  \"\"\"\n",
        "  rle_code = [int(i) for i in rle_code.split()]\n",
        "  pixels = [(pixel_position % 768, pixel_position // 768) \n",
        "                for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n",
        "                for pixel_position in range(start, start + length)]\n",
        "  return pixels\n",
        "\n",
        "def pixels_to_mask(pixels):\n",
        "  \"\"\"\n",
        "  project the pixels onto a canvas of 768*768\n",
        "\n",
        "  1. create a sparse tensor with the decoded pixels\n",
        "  2. change to dense tensor\n",
        "  3. add a dimension -> to make the dimensions: (768,768,1)\n",
        "  \"\"\"\n",
        "  canvas = np.zeros((768, 768))\n",
        "\n",
        "  canvas[tuple(zip(*pixels))] = 1\n",
        "\n",
        "  return torch.as_tensor(np.expand_dims(canvas, axis=0), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "nnZpFYSMv0yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        # transforms.RandomResizedCrop(224),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "target_transform = {\n",
        "    'train': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ])\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ])\n",
        "}\n",
        "\n",
        "paths = {'train':'data/train', 'val':'data/val'}"
      ],
      "metadata": {
        "id": "Ucy1kYBtBvHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {'train': CustomDataset(paths['train'], train_paths, transform['train'], target_transform['train']),\n",
        "            'val': CustomDataset(paths['val'], val_paths, transform['val'], target_transform['val'])\n",
        "            }\n",
        "\n",
        "dataloaders = {'train': torch.utils.data.DataLoader(datasets['train'], batch_size=16, shuffle=True),\n",
        "               'val':torch.utils.data.DataLoader(datasets['val'], batch_size=16) \n",
        "               }\n",
        "               \n",
        "dataset_sizes = {'train': len(train_paths)*128*128,\n",
        "                 'val': len(val_paths)*128*128\n",
        "                 }"
      ],
      "metadata": {
        "id": "p6fbyQqnyans"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "MK3rPYGyywuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "mxk2KgmqmztA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, scheduler, EPOCHS):\n",
        "  \"\"\"\n",
        "  for i in epochs:\n",
        "    train phase:\n",
        "      pass the inputs through the net\n",
        "      compute the loss\n",
        "      computer gradients\n",
        "      update weights\n",
        "    \n",
        "    val phase:\n",
        "      passe the inputs through the net\n",
        "      compute the loss\n",
        "\n",
        "  returns:\n",
        "    save the weights of the best model\n",
        "  \"\"\"\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f'epoch: {epoch}/{EPOCHS}:')\n",
        "    print('-'*10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if(phase =='train'):\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_pixels_count = np.prod(list(inputs.shape))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # _, preds = torch.max(outputs, 1, keepdim=True)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          if(phase == 'train'):\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_pixels_count \n",
        "        \n",
        "        running_corrects += (labels.data == (outputs > 0.5)).sum()\n",
        "\n",
        "      if(phase == 'train'):\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]   # / total_pixels_dataset\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]  # / total_pixels_dataset\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      if(phase == 'val' and epoch_acc > best_acc):\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "    print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hBeGxjocm1Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = UNet(1)\n",
        "\n",
        "unet = unet.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(unet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "BYwx6vXhST3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(unet, optimizer, criterion, exp_lr_scheduler, EPOCHS)"
      ],
      "metadata": {
        "id": "V84sskZ_nvrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(unet, 'unet.pt')"
      ],
      "metadata": {
        "id": "ZBykUMU1TC3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}