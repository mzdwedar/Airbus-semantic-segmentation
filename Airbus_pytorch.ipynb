{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airbus_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqPBFCGDMUw+vK5wIXdcYi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mziad97/Airbus-semantic-segmentation-pytorch/blob/main/Airbus_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plan"
      ],
      "metadata": {
        "id": "gcVBaDIa3PoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* build the model\n",
        "* input dimension ? D\n",
        "* concatenate layer? D\n",
        "* make the transformation with CPU, train with GPU? D\n",
        "* data dir: data -> train, test? D\n",
        "* train\n",
        "* augmentation? \n",
        "* use more images?\n",
        "\n"
      ],
      "metadata": {
        "id": "DFawKm8a3TSE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Z51lHzbOX64D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copyfile\n",
        "import copy\n",
        "import time \n",
        "\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Model"
      ],
      "metadata": {
        "id": "lTaF_aP4tYL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d_Block(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv2d = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size) , padding='same'),\n",
        "      nn.ReLU(inplace=True), \n",
        "      nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(kernel_size, kernel_size), padding='same'),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "    def foward(self, x):\n",
        "\n",
        "      return self.conv2d(x)\n",
        "\n",
        "\n",
        "class Encoder_Block(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, pool_size=(2,2) , dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.max_pool = nn.MaxPool2d(pool_size)\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "\n",
        "  def foward(self, x):\n",
        "    f = self.conv2d_block(x)\n",
        "    P = self.max_pool(f)\n",
        "    P = self.dropout(P)\n",
        "\n",
        "    return f, P\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_block_1 = Encoder_Block(in_channels=3, out_channels=64)\n",
        "    self.encoder_block_2 = Encoder_Block(in_channels=64, out_channels=128)\n",
        "    self.encoder_block_3 = Encoder_Block(in_channels=128, out_channels=256)\n",
        "    self.encoder_block_4 = Encoder_Block(in_channels=256, out_channels=512)\n",
        "\n",
        "  def forward(self, x):\n",
        "    f1, P1 = self.encoder_block(x)\n",
        "    f2, P2 = self.encoder_block(P1)\n",
        "    f3, P3 = self.encoder_block(P2)\n",
        "    f4, P4 = self.encoder_block(P3)\n",
        "\n",
        "    return P4, (f1, f2, f3, f4)\n",
        "\n",
        "\n",
        "class Bottle_Neck(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2d_block = Conv2d_Block(in_channels=512, out_channels=1024)\n",
        "\n",
        "  def forward(self, x):\n",
        "    bottleneck = self.conv2d_block(x)\n",
        "    return bottleneck"
      ],
      "metadata": {
        "id": "R5aF6pGEbIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, strides=2, dropout=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv2d_block = Conv2d_Block(in_channels, out_channels)\n",
        "    self.u = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,stride=strides, padding='same')\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self, x, conv_outputs):\n",
        "    c = torch.cat([self.u(x), self.conv_outputs], 1)\n",
        "    c = self.dropout(c),\n",
        "    c = self.conv2d_block(c)\n",
        "    \n",
        "    return c\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, last_out_channels):\n",
        "    super().__init__()\n",
        "    # f1, f2, f3, f4 = convs\n",
        "    self.decoder_block_1 = Decoder_Block(in_channels=1024, out_channels=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "    self.decoder_block_2 = Decoder_Block(in_channels=512, out_channels=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "    self.decoder_block_3 = Decoder_Block(in_channels=256, out_channels=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "    self.decoder_block_4 = Decoder_Block(in_channels=128, out_channels=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "    \n",
        "    self.conv2d_output = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=64, out_channels=last_out_channels, kernel_size=(1,1)),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, convs):\n",
        "    f1, f2, f3, f4 = convs\n",
        "    c6 = self.decoder_block_1(x, f4)\n",
        "    c7 = self.decoder_block_2(c6, f3)\n",
        "    c8 = self.decoder_block_3(c7, f2)\n",
        "    c9 = self.decoder_block_4(c8, f1)\n",
        "    outputs = self.conv2d_output(c9)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "_GKIFmXVjtDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAST_OUT_CHANNELS = 1\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "  def __init__(self, LAST_OUT_CHANNELS):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder(LAST_OUT_CHANNELS)\n",
        "    self.bottle_neck = Bottle_Neck()\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_output, convs = self.encoder(x)\n",
        "    bottleneck = self.bottle_neck(encoder_output)\n",
        "\n",
        "    outputs = self.decoder(bottleneck, convs)\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "QVYkvlzisUTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unet = UNet(1)"
      ],
      "metadata": {
        "id": "1zlgMZCLN241"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data from Kaggle"
      ],
      "metadata": {
        "id": "iyQzeOqPzWUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "0l7r6jKTzgOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187c688b-5f85-4fc0-f025-eb91e227c425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 41.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=747bfa879872eeea22c38abe669fc5e54864509bf9f74d39470a71a43a45d3b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ('train_v2' not in os.listdir('.')):\n",
        "  ! kaggle competitions download -c airbus-ship-detection "
      ],
      "metadata": {
        "id": "XydsIkY7znA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f83d43-5bcb-4627-a441-4fd6a497af3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading airbus-ship-detection.zip to /content\n",
            "100% 28.6G/28.6G [09:51<00:00, 79.3MB/s]\n",
            "100% 28.6G/28.6G [09:51<00:00, 51.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "    zipObj.extract('train_ship_segmentations_v2.csv')"
      ],
      "metadata": {
        "id": "pr2B6UVr5Qjk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = pd.read_csv('train_ship_segmentations_v2.csv', index_col=0).dropna().reset_index()\n",
        "\n",
        "segments = segments.groupby(\"ImageId\")[['EncodedPixels']].agg(lambda rle_codes: ' '.join(rle_codes)).reset_index()\n",
        "\n",
        "segments = segments[:7000]"
      ],
      "metadata": {
        "id": "YA7zOUtb5yhV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "  for file in segments['ImageId'].values:\n",
        "      file = os.path.join('train_v2', file)\n",
        "      zipObj.extract(file)\n",
        "\n",
        "# ! rm airbus-ship-detection.zip"
      ],
      "metadata": {
        "id": "349Ll0gs55Zp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, val_paths = train_test_split(segments, train_size=0.90, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "D5Z21lJ67qA5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of train set: {len(train_paths)}\")\n",
        "print(f\"The number of test set: {len(val_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq6oDfc47sHE",
        "outputId": "0cb81d42-69dc-4dfb-818a-0319aea4adee"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train set: 5950\n",
            "The number of test set: 1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = train_paths.reset_index(drop=True)\n",
        "val_paths = val_paths.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_FE9zNlI7rhE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data\n",
        "! mkdir data/train\n",
        "! mkdir data/val"
      ],
      "metadata": {
        "id": "nNYCIgHRqEyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = 'train_v2'\n",
        "train_path = './data/train'\n",
        "val_path = './data/val'\n",
        "\n",
        "def build_data_dir(SOURCE, DEST, files):\n",
        "  for filename in files:\n",
        "    src = os.path.join(SOURCE, filename)\n",
        "    dest = os.path.join(DEST, filename)\n",
        "    copyfile(src, dest)\n",
        "\n",
        "build_data_dir(SOURCE, val_path, val_paths['ImageId'].values)\n",
        "build_data_dir(SOURCE, train_path, train_path['ImageId'].values)"
      ],
      "metadata": {
        "id": "Jv0oRnZvqWTy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "rXmvk6x_tvtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.utils\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "WxxfG8eFtV3x"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, images_dir, annotations, transform=None, target_transform=None):\n",
        "    self.annotations = annotations\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, self.annotations.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    segmentation = self.annotations.iloc[idx, 1]\n",
        "\n",
        "    if(self.transform):\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if(self.target_transform):\n",
        "      segmentation = self.target_transform(segmentation)\n",
        "\n",
        "    return image, segmentation"
      ],
      "metadata": {
        "id": "ZNKxR8_YkKqC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rle_to_pixels(rle_code):\n",
        "  '''\n",
        "  Transforms a RLE code string into a list of pixels of a (768, 768) canvas\n",
        "  '''\n",
        "  rle_code = [int(i) for i in rle_code.split()]\n",
        "  pixels = [(pixel_position % 768, pixel_position // 768) \n",
        "                for start, length in list(zip(rle_code[0:-1:2], rle_code[1::2])) \n",
        "                for pixel_position in range(start, start + length)]\n",
        "  return pixels\n",
        "\n",
        "def pixels_to_mask(pixels):\n",
        "  canvas = np.zeros((768, 768))\n",
        "\n",
        "  canvas[tuple(zip(*pixels))] = 1\n",
        "\n",
        "  return torch.as_tensor(np.expand_dims(canvas, axis=0), dtype=torch.uint8)"
      ],
      "metadata": {
        "id": "nnZpFYSMv0yh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((572, 572), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        # transforms.RandomResizedCrop(224),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]).to_device('cpu:0')\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((572, 572), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]).to_device('cpu:0')\n",
        "}\n",
        "\n",
        "\n",
        "target_transform = {\n",
        "    'train': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((572, 572), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ]).to_device('cpu:0')\n",
        "    ,\n",
        "    'val': transforms.Compose([\n",
        "        rle_to_pixels,\n",
        "        pixels_to_mask,\n",
        "        transforms.Resize((572, 572), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    ]).to_device('cpu:0')\n",
        "}\n",
        "\n",
        "paths = {'train':'data/train', 'val':'data/val'}"
      ],
      "metadata": {
        "id": "Ucy1kYBtBvHE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {x: CustomDataset(x, x + '_paths', transform[x], target_transform[x]) \n",
        "            for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {'train': torch.utils.data.DataLoader(datasets['train'], batch_size=32, shuffle=True),\n",
        "               'val':torch.utils.data.DataLoader(datasets['val'], batch_size=32) }\n",
        "               \n",
        "dataset_sizes = {'train': len(train_paths),\n",
        "                 'val': len(val_paths)}"
      ],
      "metadata": {
        "id": "p6fbyQqnyans"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "MK3rPYGyywuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OxTdV4u7_-mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def im_show(inp):\n",
        "#     plt.figure(figsize=(20, 10))\n",
        "#     inp = inp.numpy().transpose((1, 2, 0))\n",
        "#     # mean = np.array([0.485, 0.456, 0.406])\n",
        "#     # std = np.array([0.229, 0.224, 0.225])\n",
        "#     # inp = std * inp + mean\n",
        "#     # inp = np.clip(inp, 0, 1)\n",
        "#     plt.imshow(inp)\n",
        "\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "metadata": {
        "id": "NBb_ILIVIW3i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def seg_show(seg):\n",
        "#   plt.figure(figsize=(20, 10))\n",
        "#   # seg = seg.squeeze()\n",
        "  \n",
        "#   plt.imshow(seg)\n",
        "#   plt.pause(0.001)"
      ],
      "metadata": {
        "id": "70rkQfkaMFsD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out = torchvision.utils.make_grid(image)"
      ],
      "metadata": {
        "id": "Nz9N4A_SJFGw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im_show(out)"
      ],
      "metadata": {
        "id": "UHeFQBQnKq_J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(20,10))\n",
        "# for i in range(6):\n",
        "#   plt.subplot(1,6,i+1)\n",
        "#   plt.imshow(segmentation[i].squeeze())"
      ],
      "metadata": {
        "id": "Gl0rSPr9BO-0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "mxk2KgmqmztA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.\n",
        "SGD = torch.optium.SGD()"
      ],
      "metadata": {
        "id": "ZIJaHBH0jVCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, scheduler, EPOCHS):\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f'epoch: {epoch}/{EPOCHS}:')\n",
        "    print('-'*10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if(phase =='train'):\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to_device('gpu:0')\n",
        "        labels = labels.to_device('gpu:0')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(preds, labels)\n",
        "\n",
        "          if(phase == 'train'):\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss = loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      \n",
        "      if(phase == 'train'):\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      if(phase == 'val' and epoch_acc > best_acc):\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "    print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hBeGxjocm1Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = UNet(1)\n",
        "\n",
        "unet = unet.to_device('gpu:0')\n",
        "\n",
        "criterion = nn.BCELOSS()\n",
        "\n",
        "optimizer = optim.SGD(unet.parameter(), lr=0.001, momentum=0.9)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "BYwx6vXhST3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}